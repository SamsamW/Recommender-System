{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, NMF, KNNWithMeans, SVDpp\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy as sup_accuracy\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import KNNBasic\n",
    "from surprise import accuracy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.set_option('display.width',1000)\n",
    "torch.manual_seed(2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Destination', 'Keywords', 'Country'], dtype='object')\n",
      "Index(['userId', 'username', 'gender', 'age', 'occupation', 'destination',\n",
      "       'rating', 'interests'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"/Users/jianing/Documents/Study/Recsys/Hands-On-Recommender-System/data/travel_cities_info.csv\")\n",
    "user_ratings_df = pd.read_csv(\"/Users/jianing/Documents/Study/Recsys/Hands-On-Recommender-System/data/users_destinations.csv\")\n",
    "\n",
    "# data = data.iloc[:10000,]\n",
    "print(cities_df.columns)\n",
    "print(user_ratings_df.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianing/miniconda3/envs/recsystest/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['Jakarta', 'Vancouver', 'Munich'], 2: ['Bangkok', 'Marrakech', 'Oslo'], 3: ['Colombo', 'Bangalore', 'Budapest'], 4: ['Kuala Lumpur', 'Quito', 'Mumbai'], 5: ['Amsterdam', 'Dar es Salaam', 'Reykjavik'], 6: ['Buenos Aires', 'Nairobi', 'Dubai'], 7: ['Warsaw', 'Zurich', 'Amsterdam'], 8: ['Santiago', 'Minsk', 'Melbourne'], 9: ['Oslo', 'Orlando', 'Mexico City'], 10: ['Kathmandu', 'Lima', 'St. Petersburg'], 11: ['Rio de Janeiro', 'Punta Cana', 'Florence'], 12: ['Oslo', 'Taipei', 'Phuket'], 13: ['Marrakech', 'Krakow', 'Bogota'], 14: ['Amsterdam', 'Yerevan', 'Barcelona'], 15: ['Cairo', 'Kathmandu', 'Vienna'], 16: ['Copenhagen', 'Vancouver', 'Accra'], 17: ['Panama City', 'St. Petersburg', 'Los Angeles'], 18: ['Kiev', 'San Jose (Costa Rica)', 'Doha'], 19: ['Belgrade', 'San Jose (Costa Rica)', 'Kathmandu'], 20: ['Quito', 'Bucharest', 'Yerevan'], 21: ['Kathmandu', 'Panama City', 'Phuket'], 22: ['Johannesburg', 'St. Petersburg', 'Shanghai'], 23: ['Bucharest', 'Geneva', 'Yerevan'], 24: ['Dubai', 'San Jose (Costa Rica)', 'Manila'], 25: ['Muscat', 'San Jose (Costa Rica)', 'Yerevan'], 26: ['Punta Cana', 'New Delhi', 'Manila'], 27: ['San Jose (Costa Rica)', 'Toronto', 'Dubai'], 28: ['Lagos', 'Orlando', 'Caracas'], 29: ['Toronto', 'Muscat', 'Helsinki'], 30: ['Barcelona', 'Bucharest', 'Yerevan'], 31: ['Islamabad', 'St. Petersburg', 'Barcelona'], 32: ['Montreal', 'Las Vegas', 'Cairo'], 33: ['Bucharest', 'Belgrade', 'Montreal'], 34: ['Cairo', 'Mumbai', 'Nairobi'], 35: ['Muscat', 'Montreal', 'Lagos'], 36: ['Krakow', 'Beijing', 'Dar es Salaam'], 37: ['Muscat', 'Santiago', 'Toronto'], 38: ['Lisbon', 'Oslo', 'Los Angeles'], 39: ['Amsterdam', 'Milan', 'Singapore'], 40: ['Seoul', 'Phuket', 'Barcelona'], 41: ['Nairobi', 'Tbilisi', 'Amsterdam'], 42: ['Copenhagen', 'Bangalore', 'Muscat'], 43: ['Amsterdam', 'Warsaw', 'Prague'], 44: ['Manila', 'Dubai', 'Santiago'], 45: ['Los Angeles', 'Islamabad', 'Panama City'], 46: ['Islamabad', 'Kiev', 'Doha'], 47: ['Orlando', 'Bangkok', 'Lagos'], 48: ['Nairobi', 'Kathmandu', 'Miami'], 49: ['Hong Kong', 'Mexico City', 'Lisbon'], 50: ['Mumbai', 'Marrakech', 'Muscat'], 51: ['Amsterdam', 'Florence', 'Dubai'], 52: ['Las Vegas', 'Hong Kong', 'Dublin'], 53: ['Melbourne', 'Dubai', 'Belgrade'], 54: ['Athens', 'Istanbul', 'Montreal'], 55: ['Phuket', 'Stockholm', 'Panama City'], 56: ['Punta Cana', 'Rio de Janeiro', 'Miami'], 57: ['Phuket', 'Taipei', 'Caracas'], 58: ['Oslo', 'Lisbon', 'Bangkok'], 59: ['Mexico City', 'Lisbon', 'Nairobi'], 60: ['Panama City', 'Bogota', 'Oslo'], 61: ['Miami', 'Santiago', 'Minsk'], 62: ['Minsk', 'Dubai', 'Belgrade'], 63: ['Kathmandu', 'Vancouver', 'Venice'], 64: ['Jerusalem', 'Warsaw', 'Zurich'], 65: ['Yerevan', 'Zurich', 'Guatemala City'], 66: ['Hong Kong', 'Milan', 'Lima'], 67: ['Brussels', 'Mexico City', 'Mumbai'], 68: ['Colombo', 'San Francisco', 'Lisbon'], 69: ['Melbourne', 'Santiago', 'Colombo'], 70: ['Manila', 'Ho Chi Minh City', 'Bangalore'], 71: ['Lima', 'Moscow', 'Panama City'], 72: ['Bucharest', 'Cairo', 'Mexico City'], 73: ['Punta Cana', 'Tel Aviv', 'Stockholm'], 74: ['Islamabad', 'Venice', 'Lisbon'], 75: ['Vancouver', 'Cairo', 'Punta Cana'], 76: ['Bucharest', 'Helsinki', 'Yerevan'], 77: ['Kathmandu', 'Krakow', 'Prague'], 78: ['Kathmandu', 'Cairo', 'Mexico City'], 79: ['Mumbai', 'Athens', 'Helsinki'], 80: ['Cairo', 'Quito', 'Bangkok'], 81: ['Copenhagen', 'St. Petersburg', 'Venice'], 82: ['Miami', 'Accra', 'Punta Cana'], 83: ['Rio de Janeiro', 'Minsk', 'Barcelona'], 84: ['Colombo', 'Krakow', 'Islamabad'], 85: ['Barcelona', 'Istanbul', 'Colombo'], 86: ['Warsaw', 'Accra', 'Reykjavik'], 87: ['Melbourne', 'Minsk', 'Kathmandu'], 88: ['Edinburgh', 'Dubai', 'Madrid'], 89: ['Barcelona', 'Muscat', 'Montreal'], 90: ['Abu Dhabi', 'Kuala Lumpur', 'Havana'], 91: ['Beijing', 'Caracas', 'Bangalore'], 92: ['New Delhi', 'Montreal', 'Marrakech'], 93: ['Dubai', 'Amsterdam', 'Panama City'], 94: ['Moscow', 'Las Vegas', 'Marrakech'], 95: ['Montreal', 'Johannesburg', 'Muscat'], 96: ['Guatemala City', 'Krakow', 'Singapore'], 97: ['Lagos', 'Muscat', 'Montreal'], 98: ['Barcelona', 'Athens', 'Colombo'], 99: ['Islamabad', 'Venice', 'San Jose (Costa Rica)'], 100: ['Milan', 'Hong Kong', 'Zurich']}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 创建城市关键词向量\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(', '))\n",
    "city_vectors = vectorizer.fit_transform(cities_df['Keywords'])\n",
    "\n",
    "# 创建每个用户的偏好向量\n",
    "user_preferences = []\n",
    "for user_id in user_ratings_df['userId'].unique():\n",
    "    user_data = user_ratings_df[user_ratings_df['userId'] == user_id]\n",
    "    user_profile = user_data.merge(cities_df, left_on='destination', right_on='Destination', how='left')\n",
    "    user_keywords_weights = user_profile.apply(lambda row: row['rating'] * city_vectors.getrow(cities_df.index[cities_df['Destination'] == row['destination']].tolist()[0]).toarray(), axis=1)\n",
    "    user_profile_vector = sum(user_keywords_weights) / len(user_data)\n",
    "    user_preferences.append((user_id, user_profile_vector))\n",
    "\n",
    "# 根据用户偏好和未评分的城市推荐新的城市\n",
    "recommendations = {}\n",
    "for user_id, user_pref_vector in user_preferences:\n",
    "    similarity_scores = cosine_similarity(user_pref_vector, city_vectors)[0]\n",
    "    recommended_city_indices = similarity_scores.argsort()[::-1]\n",
    "    recommended_cities = [cities_df.iloc[idx]['Destination'] for idx in recommended_city_indices if cities_df.iloc[idx]['Destination'] not in user_ratings_df[user_ratings_df['userId'] == user_id]['destination'].tolist()]\n",
    "    recommendations[user_id] = recommended_cities[:3] \n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# 创建城市关键词向量\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(', '))\n",
    "city_vectors = vectorizer.fit_transform(cities_df['Keywords']).toarray()\n",
    "\n",
    "# 创建每个用户的偏好向量\n",
    "user_preferences = []\n",
    "for user_id in user_ratings_df['userId'].unique():\n",
    "    user_data = user_ratings_df[user_ratings_df['userId'] == user_id]\n",
    "    user_profile = user_data.merge(cities_df, left_on='destination', right_on='Destination', how='left')\n",
    "    user_keywords_weights = user_profile.apply(lambda row: row['rating'] * city_vectors[cities_df.index[cities_df['Destination'] == row['destination']].tolist()[0]], axis=1)\n",
    "    user_profile_vector = sum(user_keywords_weights) / len(user_data)\n",
    "    user_preferences.append((user_id, user_profile_vector))\n",
    "\n",
    "# 使用KNN进行推荐\n",
    "recommendations = {}\n",
    "knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn.fit(city_vectors)\n",
    "\n",
    "for user_id, user_pref_vector in user_preferences:\n",
    "    distances, indices = knn.kneighbors([user_pref_vector], n_neighbors=3)\n",
    "    recommended_cities = [cities_df.iloc[idx]['Destination'] for idx in indices[0] if cities_df.iloc[idx]['Destination'] not in user_ratings_df[user_ratings_df['userId'] == user_id]['destination'].tolist()]\n",
    "    recommendations[user_id] = recommended_cities\n",
    "\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [], 2: [], 3: ['Colombo', 'Bangalore'], 4: [], 5: [], 6: ['Buenos Aires'], 7: ['Warsaw'], 8: ['Santiago'], 9: [], 10: ['Kathmandu'], 11: ['Rio de Janeiro'], 12: ['Oslo'], 13: [], 14: [], 15: ['Cairo'], 16: ['Copenhagen'], 17: ['Panama City'], 18: ['Kiev'], 19: ['Belgrade'], 20: ['Quito'], 21: [], 22: [], 23: ['Bucharest'], 24: ['Dubai'], 25: ['Muscat'], 26: [], 27: [], 28: ['Lagos', 'Orlando'], 29: ['Toronto', 'Muscat'], 30: ['Barcelona', 'Bucharest'], 31: ['Islamabad'], 32: ['Montreal'], 33: ['Bucharest'], 34: [], 35: ['Muscat', 'Montreal'], 36: [], 37: [], 38: ['Lisbon', 'Oslo'], 39: ['Amsterdam'], 40: [], 41: ['Nairobi'], 42: ['Copenhagen'], 43: [], 44: [], 45: ['Los Angeles'], 46: ['Islamabad'], 47: ['Bangkok'], 48: [], 49: ['Hong Kong'], 50: ['Mumbai', 'Marrakech'], 51: ['Amsterdam'], 52: ['Las Vegas'], 53: ['Melbourne'], 54: [], 55: ['Phuket'], 56: [], 57: [], 58: [], 59: ['Mexico City'], 60: ['Panama City'], 61: ['Miami'], 62: [], 63: [], 64: ['Jerusalem'], 65: ['Yerevan'], 66: [], 67: [], 68: [], 69: ['Melbourne', 'Santiago'], 70: ['Manila'], 71: ['Lima'], 72: ['Bucharest'], 73: ['Punta Cana'], 74: ['Islamabad'], 75: ['Vancouver'], 76: ['Bucharest'], 77: [], 78: [], 79: ['Mumbai'], 80: ['Cairo', 'Quito'], 81: [], 82: ['Miami'], 83: [], 84: [], 85: ['Barcelona'], 86: [], 87: [], 88: [], 89: ['Barcelona'], 90: ['Abu Dhabi'], 91: ['Beijing'], 92: [], 93: ['Dubai'], 94: [], 95: [], 96: ['Guatemala City'], 97: [], 98: [], 99: ['Islamabad', 'Venice'], 100: ['Milan']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianing/miniconda3/envs/recsystest/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# 创建城市关键词向量\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(', '))\n",
    "city_vectors = vectorizer.fit_transform(cities_df['Keywords']).toarray()\n",
    "\n",
    "# 创建用户兴趣向量\n",
    "interests_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(', '))\n",
    "user_interests = user_ratings_df.groupby('userId')['interests'].apply(lambda x: ', '.join(set(', '.join(x).split(', ')))).reset_index()\n",
    "interests_vectors = interests_vectorizer.fit_transform(user_interests['interests']).toarray()\n",
    "\n",
    "# 创建每个用户的综合偏好向量\n",
    "user_preferences = []\n",
    "for user_id in user_ratings_df['userId'].unique():\n",
    "    user_data = user_ratings_df[user_ratings_df['userId'] == user_id]\n",
    "    user_profile = user_data.merge(cities_df, left_on='destination', right_on='Destination', how='left')\n",
    "    user_keywords_weights = user_profile.apply(lambda row: row['rating'] * city_vectors[cities_df.index[cities_df['Destination'] == row['destination']].tolist()[0]], axis=1)\n",
    "    user_profile_vector = sum(user_keywords_weights) / len(user_data)\n",
    "    \n",
    "    # 获取用户兴趣向量\n",
    "    if user_id in user_interests['userId'].values:\n",
    "        user_interest_vector = interests_vectors[user_interests[user_interests['userId'] == user_id].index[0]]\n",
    "    else:\n",
    "        user_interest_vector = np.zeros_like(interests_vectors[0])\n",
    "    \n",
    "    # 合并用户偏好向量和兴趣向量\n",
    "    combined_vector = np.concatenate((user_profile_vector, user_interest_vector))\n",
    "    user_preferences.append((user_id, combined_vector))\n",
    "\n",
    "# 使用KNN进行推荐\n",
    "recommendations = {}\n",
    "knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn.fit(np.concatenate((city_vectors, np.zeros((len(city_vectors), interests_vectors.shape[1]))), axis=1))\n",
    "\n",
    "for user_id, user_pref_vector in user_preferences:\n",
    "    distances, indices = knn.kneighbors([user_pref_vector], n_neighbors=3)\n",
    "    recommended_cities = [cities_df.iloc[idx]['Destination'] for idx in indices[0] if cities_df.iloc[idx]['Destination'] not in user_ratings_df[user_ratings_df['userId'] == user_id]['destination'].tolist()]\n",
    "    recommendations[user_id] = recommended_cities[:3]  # 只推荐前3个城市\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0         1        2         3         4    5    6    7         8    9    \\\n",
      "0  0.0  0.000000  0.00000  0.199281  0.152250  0.0  0.0  0.0  0.000000  0.0   \n",
      "1  0.0  0.000000  0.25358  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
      "2  0.0  0.000000  0.00000  0.000000  0.107741  0.0  0.0  0.0  0.000000  0.0   \n",
      "3  0.0  0.000000  0.00000  0.000000  0.000000  0.0  0.0  0.0  0.140595  0.0   \n",
      "4  0.0  0.269808  0.00000  0.000000  0.103217  0.0  0.0  0.0  0.000000  0.0   \n",
      "\n",
      "   ...       110       111       112       113  114       115  116  117  \\\n",
      "0  ...  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0   \n",
      "1  ...  0.000000  0.000000  0.276498  0.000000  0.0  0.125348  0.0  0.0   \n",
      "2  ...  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0   \n",
      "3  ...  0.000000  0.270727  0.000000  0.270727  0.0  0.000000  0.0  0.0   \n",
      "4  ...  0.269808  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0   \n",
      "\n",
      "        118       119  \n",
      "0  0.000000  0.000000  \n",
      "1  0.000000  0.000000  \n",
      "2  0.281635  0.000000  \n",
      "3  0.000000  0.270727  \n",
      "4  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 120 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'username'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m user_profiles \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(user_ratings_df, tfidf_df, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcityId\u001b[39m\u001b[38;5;124m'\u001b[39m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m user_profiles\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcityId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m X\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m user_profiles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recsystest/lib/python3.12/site-packages/pandas/core/indexes/base.py:1097\u001b[0m, in \u001b[0;36mIndex.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_sequence(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;66;03m# GH#13149 specifically use astype_array instead of astype\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# pass copy=False because any copying will be done in the astype above\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m result \u001b[38;5;241m=\u001b[39m Index(new_values, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39mnew_values\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsystest/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/recsystest/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'username'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "cities_df['Keywords'] = cities_df['Keywords'].fillna('')\n",
    "tfidf_matrix = tfidf.fit_transform(cities_df['Keywords'])\n",
    "\n",
    "# 将TF-IDF矩阵转换为DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=cities_df.index.tolist())\n",
    "print(tfidf_df.head())\n",
    "\n",
    "city_encoder = LabelEncoder()\n",
    "ity_encoder = LabelEncoder()\n",
    "cities_df['cityId'] = city_encoder.fit_transform(cities_df['Destination'])\n",
    "user_ratings_df['cityId'] = city_encoder.transform(user_ratings_df['destination'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 假设ratings_df已经包含了用户评分数据\n",
    "user_profiles = pd.merge(user_ratings_df, tfidf_df, left_on='cityId', right_index=True)\n",
    "\n",
    "X = user_profiles.drop(['userId', 'cityId', 'rating'], axis=1)\n",
    "X.columns = X.columns.astype(int)\n",
    "y = user_profiles['rating']\n",
    "\n",
    "# 训练模型\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "def recommend_cities(user_id, n_recommendations):\n",
    "    # 假设所有城市特征已经是tfidf_df\n",
    "    user_rated_cities = user_ratings_df[user_ratings_df['userId'] == user_id]['cityId']\n",
    "    available_cities = cities_df[~cities_df['cityId'].isin(user_rated_cities)]\n",
    "\n",
    "    # 预测评分\n",
    "    predictions = model.predict(tfidf_df.loc[available_cities.index])\n",
    "    recommended_idx = np.argsort(predictions)[-n_recommendations:]\n",
    "\n",
    "    # 返回推荐城市\n",
    "    return cities_df.iloc[recommended_idx]['Destination']\n",
    "\n",
    "# 为用户推荐新城市\n",
    "print(recommend_cities(1, 5))\n",
    "\n",
    "\n",
    "# # 计算所有城市之间的相似度\n",
    "# cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# # 城市名称到索引的映射\n",
    "# city_to_index = {city: idx for idx, city in enumerate(cities_df['Destination'])}\n",
    "\n",
    "# # 找到用户评分城市的索引\n",
    "# user_indices = [city_to_index[city] for city in user_ratings['City']]\n",
    "\n",
    "# # 计算这些城市与所有其他城市的平均相似度得分\n",
    "# sim_scores = np.mean(cosine_sim[user_indices], axis=0)\n",
    "\n",
    "# # 除去已评分的城市\n",
    "# sim_scores[user_indices] = 0\n",
    "\n",
    "# # 获取最高得分的城市索引\n",
    "# recommended_city_indices = np.argsort(sim_scores)[-5:][::-1]\n",
    "\n",
    "# 输出推荐的城市\n",
    "# recommended_cities = cities_df['Destination'].iloc[recommended_city_indices]\n",
    "# print(recommended_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DenseFeature.__init__() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m     20\u001b[0m user_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(user_ratings_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     22\u001b[0m user_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mDenseFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterests_tfidf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# 假设interests的TF-IDF向量维度是100\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     SparseFeature(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39muser_encoder\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39msize, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m     25\u001b[0m     DenseFeature(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     28\u001b[0m item_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     SparseFeature(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39mcity_encoder\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39msize, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m     30\u001b[0m     DenseFeature(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords_tfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# 假设keywords的TF-IDF向量维度是100\u001b[39;00m\n\u001b[1;32m     31\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: DenseFeature.__init__() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_rechub.basic.features import SparseFeature, DenseFeature\n",
    "\n",
    "# 对城市关键词进行TF-IDF处理\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "cities_df['keywords_tfidf'] = list(tfidf_vectorizer.fit_transform(cities_df['Keywords']).toarray())\n",
    "keywords_features = tfidf_vectorizer.fit_transform(cities_df['Keywords']).toarray()\n",
    "interests_features = tfidf_vectorizer.fit_transform(user_ratings_df['interests']).toarray()\n",
    "\n",
    "# 对用户ID和城市名称进行编码\n",
    "user_encoder = LabelEncoder()\n",
    "user_ratings_df['userId'] = user_encoder.fit_transform(user_ratings_df['userId'])\n",
    "\n",
    "city_encoder = LabelEncoder()\n",
    "cities_df['cityId'] = city_encoder.fit_transform(cities_df['Destination'])\n",
    "user_ratings_df['cityId'] = city_encoder.transform(user_ratings_df['destination'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "user_ratings_df['age'] = scaler.fit_transform(user_ratings_df[['age']])\n",
    "\n",
    "user_features = [\n",
    "    DenseFeature(\"interests_tfidf\", dim=100),  # 假设interests的TF-IDF向量维度是100\n",
    "    SparseFeature(\"user_id\", vocab_size=user_encoder.classes_.size, embed_dim=16),\n",
    "    DenseFeature(\"age\", dim=1)\n",
    "]\n",
    "\n",
    "item_features = [\n",
    "    SparseFeature(\"city_id\", vocab_size=city_encoder.classes_.size, embed_dim=16),\n",
    "    DenseFeature(\"keywords_tfidf\", dim=100)  # 假设keywords的TF-IDF向量维度是100\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_rechub.basic.features import SparseFeature, SequenceFeature\n",
    "\n",
    "# embed_dim 是指定 LabelEncoder 的维度，会通过训练来自动学习到合适的 Lookup table\n",
    "user_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in user_cols\n",
    "]\n",
    "user_features += [\n",
    "    SequenceFeature(\"hist_movie_id\", vocab_size=feature_max_idx[\"movie_id\"], embed_dim=16, pooling=\"mean\", shared_with=\"movie_id\") # mean pooling，会对历史观影的 embedding 做平均运算\n",
    "]\n",
    "\n",
    "item_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in item_cols\n",
    "]\n",
    "\n",
    "print(user_features[1].name)\n",
    "print(user_features[1].get_embedding_layer())\n",
    "print(user_features[1].get_embedding_layer()._parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DSSM.__init__() missing 3 required positional arguments: 'item_features', 'user_params', and 'item_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_rechub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatchDataGenerator\n\u001b[1;32m      5\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_tower\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m)},\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_tower\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m)},\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDSSM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m data_gen \u001b[38;5;241m=\u001b[39m MatchDataGenerator(user_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m'\u001b[39m, item_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcityId\u001b[39m\u001b[38;5;124m'\u001b[39m, label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     13\u001b[0m                               user_max_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     15\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m data_gen\u001b[38;5;241m.\u001b[39mgenerate_dataloader(df_train\u001b[38;5;241m=\u001b[39muser_ratings_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: DSSM.__init__() missing 3 required positional arguments: 'item_features', 'user_params', and 'item_params'"
     ]
    }
   ],
   "source": [
    "from torch_rechub.models.matching import DSSM\n",
    "from torch_rechub.trainers import MatchTrainer\n",
    "from torch_rechub.utils.data import MatchDataGenerator\n",
    "\n",
    "model_config = {\n",
    "    \"user_tower\": {\"embedding_dim\": 128, \"hidden_units\": (64, 32)},\n",
    "    \"item_tower\": {\"embedding_dim\": 128, \"hidden_units\": (64, 32)},\n",
    "}\n",
    "\n",
    "model = DSSM(user_features, item_features, model_config)\n",
    "\n",
    "data_gen = MatchDataGenerator(user_col='userId', item_col='cityId', label_col='rating', \n",
    "                              user_max_seq=10, batch_size=64)\n",
    "\n",
    "train_loader, val_loader = data_gen.generate_dataloader(df_train=user_ratings_df)\n",
    "\n",
    "trainer = MatchTrainer(model, train_loader, val_loader, \n",
    "                       optimizer=\"adam\", \n",
    "                       epochs=10, \n",
    "                       lr=0.001,\n",
    "                       verbose=1)\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 根据之前处理的数据拿到Dataloader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dg \u001b[38;5;241m=\u001b[39m MatchDataGenerator(x\u001b[38;5;241m=\u001b[39m\u001b[43mx_train\u001b[49m, y\u001b[38;5;241m=\u001b[39my_train)\n\u001b[1;32m      3\u001b[0m train_dl, test_dl, item_dl \u001b[38;5;241m=\u001b[39m dg\u001b[38;5;241m.\u001b[39mgenerate_dataloader(test_user, all_item, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 定义模型\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 根据之前处理的数据拿到Dataloader\n",
    "dg = MatchDataGenerator(x=x_train, y=y_train)\n",
    "train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n",
    "\n",
    "# 定义模型\n",
    "model = DSSM(user_features, item_features, temperature=0.02,  # 在归一化之后的向量计算內积之后，乘一个固定的超参 r ，论文中命名为温度系数。归一化后如果不乘 temperature，模型无法收敛\n",
    "             user_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',  # important!!\n",
    "             },\n",
    "             item_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',  # important!!\n",
    "             })\n",
    "\n",
    "# 模型训练器\n",
    "trainer = MatchTrainer(model,\n",
    "                       mode=0,  # 同上面的mode，需保持一致\n",
    "                       optimizer_params={\n",
    "                           \"lr\": 1e-4,\n",
    "                           \"weight_decay\": 1e-6\n",
    "                       },\n",
    "                       n_epoch=10,\n",
    "                       device='cpu',\n",
    "                       model_path=save_dir)\n",
    "\n",
    "# 开始训练\n",
    "trainer.fit(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_rechub.utils.match import Annoy\n",
    "from torch_rechub.basic.metric import topk_metrics\n",
    "\n",
    "def match_evaluation(user_embedding, item_embedding, test_user, all_item, user_col='user_id', item_col='movie_id',\n",
    "                     raw_id_maps=\"./raw_id_maps.npy\", topk=10):\n",
    "    print(\"evaluate embedding matching on test data\")\n",
    "    annoy = Annoy(n_trees=10)\n",
    "    annoy.fit(item_embedding)\n",
    "\n",
    "    #for each user of test dataset, get ann search topk result\n",
    "    print(\"matching for topk\")\n",
    "    user_map, item_map = np.load(raw_id_maps, allow_pickle=True)\n",
    "    match_res = collections.defaultdict(dict)  # user id -> predicted item ids\n",
    "    for user_id, user_emb in zip(test_user[user_col], user_embedding):\n",
    "        items_idx, items_scores = annoy.query(v=user_emb, n=topk)  #the index of topk match items\n",
    "        match_res[user_map[user_id]] = np.vectorize(item_map.get)(all_item[item_col][items_idx])\n",
    "\n",
    "    #get ground truth\n",
    "    print(\"generate ground truth\")\n",
    "\n",
    "    data = pd.DataFrame({user_col: test_user[user_col], item_col: test_user[item_col]})\n",
    "    data[user_col] = data[user_col].map(user_map)\n",
    "    data[item_col] = data[item_col].map(item_map)\n",
    "    user_pos_item = data.groupby(user_col).agg(list).reset_index()\n",
    "    ground_truth = dict(zip(user_pos_item[user_col], user_pos_item[item_col]))  # user id -> ground truth\n",
    "\n",
    "    print(\"compute topk metrics\")\n",
    "    out = topk_metrics(y_true=ground_truth, y_pred=match_res, topKs=[topk])\n",
    "    return out\n",
    "\n",
    "user_embedding = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=save_dir)\n",
    "item_embedding = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=save_dir)\n",
    "\n",
    "match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=10, raw_id_maps=save_dir+\"raw_id_maps.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsystest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
